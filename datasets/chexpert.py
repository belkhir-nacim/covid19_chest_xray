# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qZxDuQ1BOuLnYy2dax-WjsFP8BVsmfRB
"""

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset
import os
from PIL import Image
from sklearn.model_selection import train_test_split
from torchvision.datasets.utils import download_and_extract_archive

class CheXpertDataset(Dataset):
    """
    Dataset class for the chexpert dataset
    
    Args:
        path_to_main_folder: path where the extracted chexpert data is located
        fold: choose 'train', 'valid' or 'test'
        transform: torchvision transforms to be applied to raw images
        uncertainty: the uncertainty method to be used in training
    """
    url = 'https://us13.mailchimp.com/mctx/clicks?url=http%3A%2F%2Fdownload.cs.stanford.edu%2Fdeep%2FCheXpert-v1.0-small.zip&h=43e9a85ba260e8c46a7b22c9ba6ae81986affde1c7a21b9da19eea30b06772be&v=1&xid=c97dbc1502&uid=55365305&pool=contact_facing&subject=CheXpert-v1.0%3A+Subscription+Confirmed'


    def __init__(self,path_to_main_folder,fold, transform=None,download=True,uncertainty="zeros",zip_file_path=None):

        self.transform = transform
        self.path_to_main_folder = path_to_main_folder
        if not os.path.exists(self.path_to_main_folder): os.makedirs(self.path_to_main_folder)
    
        self.initial_data_check(download=download, zip_file_path=zip_file_path)

        if fold == 'test':   #Use the validation set in the chexpert dataset as the test set
            self.df = pd.read_csv(os.path.join(path_to_main_folder,'CheXpert-v1.0-small','valid.csv'))
        elif fold == 'train': #Use 80% of the train set in the chexpert dataset as the train set
            self.df = pd.read_csv(os.path.join(path_to_main_folder,'CheXpert-v1.0-small','train.csv'))
            self.df, _ = train_test_split(self.df, test_size=0.2, random_state=42)
        elif fold == 'val':  #Use 20% of the train set in the chexpert dataset as the validation set
            self.df = pd.read_csv(os.path.join(path_to_main_folder,'CheXpert-v1.0-small','train.csv'))
            _, self.df = train_test_split(self.df, test_size=0.2, random_state=42)
                
        self.df = self.df.set_index("Path") #Use the path of the image directory as the index
        self.df = self.df.drop(['Sex', 'Age', 'Frontal/Lateral','AP/PA'], axis=1) #Drop these columns because we won't be needing them
        self.df = self.df.fillna(value=0) #Fill in the missing values with zeros (negative)
        
        if "zeros" in uncertainty: #If the zeros uncertainty method is used, treat all uncertain labels as zeros
            self.df = self.df.replace(-1, 0)
        elif "ones" in uncertainty: #If the ones uncertainty method is used, treat all uncertain labels as ones
            self.df = self.df.replace(-1, 1)
        
        self.PRED_LABEL = ['No Finding','Enlarged Cardiomediastinum','Cardiomegaly','Lung Opacity','Lung Lesion','Edema',
                           'Consolidation','Pneumonia','Atelectasis','Pneumothorax','Pleural Effusion','Pleural Other',
                           'Fracture','Support Devices']  #These are the 14 labels we try to predict


    def initial_data_check(self, download, zip_file_path):
        if (not os.path.exists(os.path.join(self.path_to_main_folder,'CheXpert-v1.0-small'))):
            if os.path.exists(zip_file_path):
                print('Extract file')
                extract_archive(from_path=zip_file_path, to_path=self.path_to_main_folder)
            elif download == True:
                print("Start Downloading CheXpert-v1.0-small'")
                self.download_and_extract()
            else:
                raise IOError('Data are not available and not zip file is provided or download is not activated')
        else:
            pass
            

    def download_and_extract(self):
        filename='CheXpert-v1.0-small.zip'
        print("Download: START")
        download_and_extract_archive(url=self.url,
                                     download_root=self.path_to_main_folder,
                                     filename=filename, remove_finished=False)
        print("Download: DONE")
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):

        image = Image.open(
            os.path.join(
                self.path_to_main_folder, 
                self.df.index[idx]))
        image = image.convert('RGB')

        label = np.zeros(len(self.PRED_LABEL), dtype=int)
        for i in range(0, len(self.PRED_LABEL)):
             # can leave zero if zero, else make one
            if(self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('int') in set([-1,1])):
                label[i] = self.df[self.PRED_LABEL[i].strip()
                                   ].iloc[idx].astype('int')

        if self.transform is not None:
            image = self.transform(image)

        return (image, label,self.df.index[idx])
      
    def getWeights(self, uncertainty='weighted_zeros'): #The weight array for weighted cross entropy methods
        
        if uncertainty == 'weighted_zeros':
          positives = self.df.sum(axis=0)[-14:] / self.df.shape[0]
          negatives = 1 - positives
          weights = 2 * negatives
          
        elif uncertainty == 'weighted_multiclass':  
          counts = self.df.count()[-14:]
          uncertains = self.df.isin([-1]).sum(axis=0)[-14:]
          positives = self.df.isin([1]).sum(axis=0)[-14:]
          negatives = self.df.isin([0]).sum(axis=0)[-14:]
          weights = [(counts - uncertains) / counts * 3/2, \
                     (counts - negatives)  / counts * 3/2, \
                     (counts - positives) / counts  * 3/2] 
          weights = np.transpose(np.array(weights))
        
        return weights
        
    def effective_num_weights(self, beta=0.9999, mode='intra'):
        num_pos_samples = []
        num_neg_samples = []


        for i in range(0, len(self.PRED_LABEL)):
            num_pos_samples.append(self.df[self.df[self.PRED_LABEL[i].strip()] == 1].shape[0])
            num_neg_samples.append(self.df[self.df[self.PRED_LABEL[i].strip()] == 0].shape[0])
            
        pos_denominator_term = 1.0 - np.power(beta, num_pos_samples);
        neg_denominator_term = 1.0 - np.power(beta, num_neg_samples);
        pos_weights = (1.0 - beta) / np.array(pos_denominator_term);
        neg_weights = (1.0 - beta) / np.array(neg_denominator_term);
        
        if mode == 'inter':
            weights_interclass_normalized = pos_weights / np.sum(pos_weights) * len(pos_weights)
            weights = weights_interclass_normalized
        
        else:
        
            #weights_intraclass_normalized = pos_weights / (pos_weights + neg_weights) * 2; Bence bu yanlış
            weights_intraclass_normalized = pos_weights / (neg_weights + 0.001)
            weights = weights_intraclass_normalized
            #weights = torch.Tensor(weights_intraclass_normalized)
            #weights = weights.cuda()
            #weights = weights.type(torch.cuda.FloatTensor)
        return weights
    
    def pos_neg_sample_nums(self):
        df = self.df[self.PRED_LABEL]
        N, C = df.shape
        pos_neg_sample_nums = np.zeros((2,C))
        pos_neg_sample_nums[0, :] = df.sum(axis=0).values
        pos_neg_sample_nums[1, :] = N - pos_neg_sample_nums[0, :]
        del df
        return pos_neg_sample_nums

# def createDatasets(path_to_main_folder, data_transforms, uncertainty):
#     # create train/val dataloaders
#     transformed_datasets = {}
#     transformed_datasets['train'] = CheXpertDataset(path_to_main_folder=path_to_main_folder, fold='train', transform=data_transforms['train'], uncertainty=uncertainty)
#     transformed_datasets['val'] = CheXpertDataset(path_to_main_folder=path_to_main_folder,fold='val',transform=data_transforms['val'],uncertainty=uncertainty)
#     transformed_datasets['test'] = CheXpertDataset(path_to_main_folder=path_to_main_folder,fold='test',transform=data_transforms['val'],uncertainty=uncertainty)
    
#     return transformed_datasets
# datasets = createDatasets(path_to_main_folder='/content/datasets',data_transforms =dict(train=None, val=None),uncertainty='ones')

